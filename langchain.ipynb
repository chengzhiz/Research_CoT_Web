{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_questions_to_txt(jsonl_path, output_txt_path):\n",
    "    # Read the .jsonl file into a pandas DataFrame\n",
    "    df = pd.read_json(jsonl_path, lines=True)\n",
    "    \n",
    "    # Extract the 'question' column\n",
    "    questions = df['question']\n",
    "    \n",
    "    # Write the questions to a .txt file, each question on a new line\n",
    "    with open(output_txt_path, 'w') as f:\n",
    "        for question in questions:\n",
    "            f.write(question + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_questions_to_txt('dev.jsonl', 'boolean_questions.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " RAG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader #load the document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter #for creating chunks from the loaded document\n",
    "from langchain_openai import OpenAIEmbeddings #for converting chunks into embeddings\n",
    "from langchain_chroma import Chroma #database for stroring the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/chapter318/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology/F24/8803_AI4DM/Proj2/FlaskApp_Template/chroma_db\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dir = os.getcwd()\n",
    "db_dir = os.path.join(dir,\"chroma_db\")\n",
    "print(db_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the text content from the .txt file and load it as langchain document\n",
    "loader = TextLoader('boolean_questions.txt')\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document chunk info:\n",
      "\n",
      "Number of document chunks: 181\n",
      "Sample chunk: \n",
      "can you wear short sleeve shirt with asu jacket\n",
      "has wisconsin ever been in the little league world series\n",
      "does damon and elena get together in season 3\n",
      "is there a player in the nfl missing a hand\n",
      "is the other boleyn girl part of a series\n",
      "is there a group called the five heartbeats\n",
      "is mount everest a part of the himalayas\n",
      "can an emt-basic start an iv\n",
      "has no 1 court at wimbledon got a roof\n",
      "has anyone come back from 3-0 in the nba finals\n",
      "do radio waves travel at the speed of light\n",
      "did anyone from the 1980 us hockey team play in the nhl\n",
      "do all triangles have at least two acute angles\n",
      "is baylor and mary hardin baylor the same school\n",
      "can you get the death penalty as a minor\n",
      "did indian football team qualified for fifa 2018\n",
      "are t rex and tyrannosaurus rex the same\n",
      "is the old panama canal still in use\n",
      "do you need a pal to possess ammunition\n",
      "do blue and pink cotton candy taste the same\n",
      "did to kill a mockingbird win an academy award\n",
      "is there such a thing as a floating island\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Split the document into chunks using text splitters \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(document)\n",
    "\n",
    "print(\"Document chunk info:\\n\")\n",
    "print(f\"Number of document chunks: {len(chunks)}\")\n",
    "print(f\"Sample chunk: \\n{chunks[3].page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x1281e6990>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create embeddings using openAI embeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "#store the embeddings and chunks into Chroma DB\n",
    "Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve and generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chapter318/opt/anaconda3/envs/py33/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_g/_764cssj41d39tz0k3_y00v80000gn/T/ipykernel_19187/1499084276.py:2: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings_used = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
      "/var/folders/_g/_764cssj41d39tz0k3_y00v80000gn/T/ipykernel_19187/1499084276.py:3: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorDB = Chroma(persist_directory=db_dir,embedding_function=embeddings_used)\n"
     ]
    }
   ],
   "source": [
    "#setting up the DB for retrieval\n",
    "embeddings_used = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectorDB = Chroma(persist_directory=db_dir,embedding_function=embeddings_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up Retriver\n",
    "retriever = vectorDB.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRetriever(dir):\n",
    "    \"\"\"\n",
    "    dir is the directory of the vector DB\n",
    "    \"\"\"\n",
    "    embeddings_used = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    vectorDB = Chroma(persist_directory=dir,embedding_function=embeddings_used)\n",
    "    retriever = vectorDB.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"Authorization\": \"Bearer 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\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "def textGeneration_langChain_RAG(user_type, retrieverDir, api_key):\n",
    "    \"\"\"\n",
    "    user_type: The type of user (e.g., children, adults, etc.).\n",
    "    retrieverDir: Directory of the vector DB with relevant boolean questions.\n",
    "    api_key: Your Hugging Face API key for authentication.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the Inference Client\n",
    "    client = InferenceClient(api_key=api_key)\n",
    "\n",
    "    # Retrieve relevant boolean questions from Chroma DB using user_type\n",
    "    retriever = getRetriever(retrieverDir)\n",
    "    \n",
    "    # Modify the query to include user_type in the context\n",
    "    query = f\"Get some boolean questions for a {user_type}.\"\n",
    "    retrieved_docs = retriever.get_relevant_documents(query)\n",
    "    \n",
    "    # Extract content from retrieved documents\n",
    "    context = \"\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "    # Create a clear and direct system prompt for generating a boolean question\n",
    "    system_prompt = (\n",
    "        \"Based on the following context, generate one complete boolean question that a {user_type} would ask:\\n\"\n",
    "        \"{context}\\n\\n\"\n",
    "        \"Make sure it's only one grammatically correct question and can be answered with yes/no.\"\n",
    "    )\n",
    "\n",
    "    # Prepare the final prompt to send to the Hugging Face API\n",
    "    final_prompt = system_prompt.format(user_type=user_type, context=context)\n",
    "\n",
    "    # Prepare the messages for the chat API\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": final_prompt}\n",
    "    ]\n",
    "\n",
    "    # Stream the response from the Hugging Face Inference Client\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"HuggingFaceTB/SmolLM2-1.7B-Instruct\", \n",
    "        messages=messages, \n",
    "        max_tokens=500,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    # Collect and print the output from the stream\n",
    "    full_response = \"\"\n",
    "    for chunk in stream:\n",
    "        full_response += chunk.choices[0].delta.content\n",
    "\n",
    "    return full_response.strip()  # Return the final response witho\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can a car run on anything other than gasoline?\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "api_key = \"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\"  # Replace with your actual Hugging Face API key\n",
    "output = textGeneration_langChain_RAG(\n",
    "    user_type=\"mom\",  # Specify the user type\n",
    "    retrieverDir=db_dir,  # Your Chroma DB directory\n",
    "    api_key=api_key  # Your Hugging Face API key\n",
    ")\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py33",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
